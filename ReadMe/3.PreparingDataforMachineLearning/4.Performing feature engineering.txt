- [Instructor] Let's look at handling missing data. In the Jupyter Notebook, we're using machine learning to impute or determine the missing data. 
-KNN or K-nearest neighbors is a machine learning algorithm that is the quick and effective method to impute missing values when your data set is small. 
-KNN identifies a sample with one or more missing values, then it identifies the K most similar samples in the training data that are complete, ie, have no missing values in some columns, and then replaces the missing value. 
-Let's use KNN to calculate or predict the missing values. I've navigated to the Jupyter Notebook and the first thing I'm going to do, I'm going to run all of the cells. I do that by clicking on run and selecting run all cells. Now let's go down to the section where we are imputing the missing data. So let's scroll down. Here we are. The first thing we'll need to do is import the KNN imputer. Next, we'll create a temporary copy of our data set, because we don't want to mess up the original, and then we'll retrieve the columns only with the numeric values. This will exclude the ocean proximity column since the data type is object, other columns are float 64. Next, we'll extract the columns that contain at least one missing value, and then we will update the temp data frame with numeric columns that have empty values. Next, we are initializing the KNN imputer. We are running the fit function to train the model, we are using the transform function to transform the data, and then we are converting the array values to a data frame with the appropriate column names. Now that we've imputed the missing data, let's remove the highly correlated features, and that section is further down in our Jupyter Notebook where we have our heat map. First, we'll visualize the correlation using the heat map. You'll see the figure function of the map plot lib library is used to set the heat map attributes like figure size which allows you to change the width and the height of the heat map. Here, we'll pass in the correlation values which we previously calculated to this heat map function, that's in the seaborn library. Next, we will use the show function of the map plot lib library to show our heat map. You'll notice here that there's high correlation. To deal with the high correlation, we'll create a relatively simple combination of the correlated features and drop the rest. We'll also create a feature that combines longitude and latitude as there is this negative correlation. Let's scroll down. Here, we are creating a new feature that is a ratio of the total rooms to households. On this line, we're creating a new feature that is the ratio of the total bedrooms to total rooms. Next, we are creating a new feature that is a ratio of the population to the households. Next, we are combining longitude and latitude into one new feature. Now that we've created these new features, let's remove the old features. We'll do that here, we are removing the correlated features, total rooms, households, total bedrooms, population, longitude, and latitude. Now we'll display the updated heat map after removing the correlations. You notice we're using the same core function, figure function, heat map function, and show function like we did before. And notice, the highly correlated features no longer exist. Now let's use one hot encoding to convert the ocean proximity field from categorical data to numeric data. Let's scroll down. The first step is to determine the unique categories for ocean proximity. The expected output is shown here, near bay, less than one hour from the ocean, inland, near ocean, and island. Now let's count the values across the columns. The expected output is shown here and it's very useful information. The next step is to use this get dummies function to replace the ocean proximity field. This generates new columns based on the possible options and places a one to indicate the value. Let's scroll down and print. Here, the first few records in our data frame. And let's scroll over. And you see here, the one is placed in the correct column. And also notice the old ocean proximity column cannot be found, it is gone. Now that our data is processed and ready, let's start the training process.



Here are the key takeaways:

Handling Missing Data: The video demonstrates using the KNN imputer to predict missing values in a dataset, which is crucial for maintaining the integrity of your models.

Dealing with Highly Correlated Features: It shows how to identify and remove highly correlated features to prevent redundancy in your model. This step is essential for improving model performance and interpretability.

Creating New Features: The instructor illustrates how to create new features by combining existing ones, such as merging longitude and latitude into a single feature or creating ratios of related features. This can lead to more meaningful insights and improve the predictive power of your models.

One Hot Encoding: The video also covers converting categorical data into numeric data using one hot encoding, which is vital for preparing your dataset for machine learning algorithms.
