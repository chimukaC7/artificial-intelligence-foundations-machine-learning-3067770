- [Instructor] Let's see the steps needed to implement pipelines in our code. First, let's review the normal machine learning code used to train a model. This is code you've seen before, which is used to train a model to predict the cost of a home. First, you obtain and load your data. Then we go through the preparation process where we understand missing data, we impute that missing data, we use visualizations to understand the relationships, we use histograms, we also use heat maps to understand the correlation. Then we did a bit of feature engineering to remove the highly correlated fields, then we encoded our categorical data, and then we trained the model. And the most important piece that I want you to understand here, when we trained our model, we used three different learning algorithms. We had a section for linear regression, We trained the model, we ran the predictions, we evaluated the model, and then we had a section for using the Random Forest Regressor learning algorithm, and then we had a section further down for XG Boost. For now, the code to pre-process your data, like imputing missing values, detecting outliers, using histograms, understanding correlations through heat maps, feature engineering and categorical encoding stays pretty much the same. However you can, in the future, integrate imputing missing values, encoding the data and additional operations into your pipeline where it makes sense. But what I really want you to see is how incorporating a pipeline makes the training and learning algorithm experimentation process more streamlined, repeatable, and efficient; giving you speed to market and reducing training costs. Now let's navigate to the code for pipelines. We are updating our code to train the model in parallel using a pipeline. The code stacks multiple pipelines to find the model with the best accuracy. Notice here we are importing the necessary pipeline and learning algorithms for linear regression, Random Forest Regressor, and X G B regressor. Next, we create the specific objects that we need for those learning algorithms and then we add those algorithms to a pipeline. Next, we create a dictionary of labels for each learning algorithm we use. This is mainly for printing out later, so that the output is easier to read and understand. Next, we loop through each learning algorithm object in the pipeline calling the fit function. Notice the training code is centralized into one call instead of multiple individual calls. After training, we loop through each train model to run the predictions to help us evaluate our model by comparing the actual values to the predicted ones. We use the score function to obtain the evaluation metric and then we print out the results. And let's take a look at those results. As you can see, the accuracy for the linear regression is at 0.56 or 56%. For the Random Forest Regressor, it is at 75%. And for X G Boost it is at 77%. This was a quick and easy way to determine that X G Boost performs the best on our data. There are additional learning algorithms we can add to the pipeline for experimentation. Pipelines helped us to reduce our training and experimentation process to just a few lines of code. Look at how much more efficient this code is. The organization helps us build quick and efficient machine learning models. Automating this process saves us time and reduces redundant pre-processing work, while making our code more efficient. I encourage you to pause the video, pull down the code, and continue to improve the efficiency by incorporating hyper-parameter tuning, imputation of missing values and encoding into your machine learning pipeline. Let us know the efficiencies you gain in the course Q and A.





Sure, let's simplify the concept of designing and building a machine learning pipeline, as shown in the “Demo: Designing and building a pipeline” video.

Imagine you're making a complex dish that requires several steps: preparing ingredients, mixing them in a specific order, cooking at the right temperature, and then plating it beautifully. Doing each step manually every time can be time-consuming and prone to mistakes. Now, imagine if you had a magical kitchen where, once you prepare your ingredients, a machine automatically takes over, mixes them in the perfect order, cooks everything just right, and even plates it for you. That's essentially what a machine learning pipeline does for building machine learning models.

Here's a step-by-step breakdown using simple terms:

Prepare Your Ingredients (Data Preparation): Just like cooking, you start with gathering and preparing your ingredients. In machine learning, this means getting your data ready - cleaning it, organizing it, and maybe transforming it so it's in the best shape for cooking (or in this case, for the model to learn from).

Choose Your Recipes (Selecting Algorithms): Depending on what you're cooking, you choose a recipe. Similarly, you select algorithms (recipes for the computer) that you think will best learn from your data to make predictions.

Let the Magic Kitchen Do Its Work (Training the Model): Instead of manually mixing ingredients and cooking, you let the pipeline (magic kitchen) automate these steps. You feed your prepared data into the pipeline, and it automatically processes the data through the algorithms you've chosen, essentially 'cooking' your model.

Taste Test (Evaluation): Just as you would taste your dish to see if it needs anything, the pipeline helps evaluate how well the model is performing. It checks if the predictions are close to what you expect.

Serve Your Dish (Making Predictions): With the model trained and evaluated, you can now use it to make predictions on new data, just like serving your dish to guests.


In the video, the instructor shows how using a pipeline makes the entire process more efficient, repeatable, and less prone to errors. Instead of manually handling each step, the pipeline automates the process, saving time and making it easier to experiment with different recipes (algorithms) to find the best one.

In essence, a machine learning pipeline automates the process of turning data into predictions, much like a magical kitchen automates the process of turning ingredients into a dish. This makes it easier, faster, and more efficient to build and refine machine learning models.